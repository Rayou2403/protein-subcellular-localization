\section{Contexte et objectif}
L'objectif est de predire la localisation subcellulaire de proteines procaryotes (6 classes)
a partir de sequences FASTA. Le projet impose une approche plus originale qu'un simple MLP, avec
une architecture qui utilise des embeddings, une couche Transformer, puis un MLP de classification.

\section{Contraintes et environnement}
Le travail a ete realise sur un ASUS TUF 15, majoritairement en CPU. Cela impose des limites de memoire et
un temps de calcul eleve pour l'extraction des embeddings. Pour rester dans ces contraintes:
\begin{itemize}
  \item limitation de la longueur maximale (\texttt{max\_len}) pour eviter les OOM ;
  \item choix d'un second embedding plus leger (ProtBert) pour les runs CPU, car ProstT5 est
  trop lourd en memoire sans GPU ;
  \item cache local des modeles HF pour eviter les retelechargements ;
  \item batch sizes ajustes (ESM-C plus grand, ProtBert plus petit si besoin).
\end{itemize}

\section{Donnees}
Les donnees proviennent de DeepLocPro, fournies au format FASTA. Le header suit le format:
\begin{verbatim}
>PROTEIN_ID|LOCATION|GRAM_TYPE|PARTITION
\end{verbatim}
Nous generons un fichier \texttt{metadata.csv} puis des splits \texttt{train/val/test}. Par defaut, les splits
sont stratifies, avec une option pour clustering par identite si MMseqs2 est installe.

\section{Pipeline de traitement}
\begin{enumerate}
  \item \textbf{Preparation des metadonnees} a partir du FASTA.
  \item \textbf{Creation des splits} leakage-safe.
  \item \textbf{Extraction des embeddings}:
    \begin{itemize}
      \item ESM-C (300M) pour un embedding principal (dimension 960).
      \item ProtBert (dimension 1024) comme second embedding sur CPU.
    \end{itemize}
  \item \textbf{Entrainement} du modele Transformer + MLP.
  \item \textbf{Evaluation} sur le split test (Accuracy, F1 macro, MCC).
\end{enumerate}

\section{Choix des embeddings et reduction du cout}
Le choix ESM-C + ProtBert est un compromis qualite/ressources. ESM-C offre un embedding performant,
mais ProstT5 demande beaucoup de memoire (modele > 11GB) et n'est pas viable sur CPU.
ProtBert est beaucoup plus leger et stable sans GPU.

Pour limiter le temps de calcul, \texttt{max\_len} est fixe a 300. Cela conserve environ 30\% des sequences
et permet d'extraire les embeddings sur CPU sans erreurs de memoire. Avec un GPU, on peut augmenter
la longueur maximale pour se rapprocher de 100\% des donnees.

\section{Architecture du modele}
Le modele respecte l'exigence \emph{embeddings $\rightarrow$ Transformer $\rightarrow$ MLP}:
\begin{itemize}
  \item projection lineaire des deux embeddings vers une dimension commune ;
  \item traitement des deux modalites comme deux tokens d'un petit Transformer encoder ;
  \item sortie du token \texttt{[CLS]} vers un MLP de classification.
\end{itemize}

\subsection*{Configuration principale}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Parametre & Valeur \\
\midrule
Embedding 1 & ESM-C 300M (dim 960) \\
Embedding 2 & ProtBert (dim 1024) \\
Pooling & meanpool \\
max\_len & 300 \\
ESM batch & 32 \\
Embed2 batch & 32 \\
\bottomrule
\end{tabular}
\caption{Configuration principale pour l'extraction et l'entrainement.}
\end{table}

\section{Entrainement}
La perte utilisee est la \emph{focal loss} avec un echantillonnage equilibre pour gerer l'imbalance des classes.
L'entrainement utilise un early stopping base sur le F1 macro.

\section{Exploration des donnees (EDA)}
Le dataset contient 11\,906 sequences. Les colonnes principales sont: \texttt{protein\_id}, \texttt{sequence},
\texttt{label}, \texttt{gram\_type}, \texttt{partition}, \texttt{split}, \texttt{seq\_len}. Aucune valeur manquante
n'a ete observee.

\subsection*{Distribution des classes}
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Classe & Nombre \\
\midrule
Cytoplasmic & 6885 \\
Cytoplasmic Membrane & 2535 \\
Extracellular & 1077 \\
Outer Membrane & 756 \\
Periplasmic & 566 \\
Cell Wall & 87 \\
\bottomrule
\end{tabular}
\caption{Distribution des classes (EDA).}
\end{table}

\subsection*{Statistiques de longueur}
La longueur des sequences est tres variable (distribution a longue traine). Statistiques principales:
\begin{itemize}
  \item moyenne: 438.4, ecart-type: 289.0
  \item min: 8, mediane: 399, max: 5627
  \item P10: 146, P25: 264, P75: 557, P90: 787, P95: 878, P99: 1296
\end{itemize}
Pour des tests rapides sur CPU, un \texttt{max\_len} entre 150 et 300 est recommande.
Avec \texttt{max\_len=300}, environ 30\% des sequences sont conservees, ce qui
reduit fortement le cout de calcul.

\subsection*{Repartition des splits}
Train: 8929, Val: 1191, Test: 1786.

\subsection*{Graphiques}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{overview.png}
\caption{Vue d'ensemble: labels, longueurs, et splits.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{sequence_length_hist.png}
\caption{Distribution des longueurs de sequences.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{split_distribution.png}
\caption{Repartition train/val/test.}
\end{figure}

\section{Resultats}
Run utilise: ESM-C + ProtBert (meanpool), \texttt{max\_len=300}, execution CPU.

\subsection*{Metriques globales}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Run & Accuracy & F1 macro & MCC \\
\midrule
Evaluation & 0.8741 & 0.8005 & 0.8227 \\
\bottomrule
\end{tabular}
\caption{Metriques globales sur le split test.}
\end{table}

\subsection*{Resultats par classe}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Classe & Precision & Recall & F1 \\
\midrule
Cytoplasmic & 0.901 & 0.846 & 0.873 \\
Cytoplasmic Membrane & 0.500 & 0.750 & 0.600 \\
Periplasmic & 0.933 & 0.925 & 0.929 \\
Outer Membrane & 0.776 & 0.839 & 0.806 \\
Extracellular & 0.706 & 0.818 & 0.758 \\
Cell Wall & 0.857 & 0.818 & 0.837 \\
\bottomrule
\end{tabular}
\caption{Precision/Recall/F1 par classe (split test).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{results_metrics.png}
\caption{Metriques globales.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{results_per_class_f1.png}
\caption{F1 par classe.}
\end{figure}

\section{Limites et pistes d'amelioration}
\begin{itemize}
  \item Monter \texttt{max\_len} et/ou utiliser un GPU pour integrer plus d'information de sequence.
  \item Remplacer ProtBert par ProstT5 sur GPU pour exploiter la structure 3D.
  \item Tester un Transformer plus profond ou un pooling different (mean/cls/both).
\end{itemize}

\section{Reproductibilite}
Commandes principales (extraits):
\begin{verbatim}
python scripts/prepare_metadata.py --fasta data/raw/graphpart_set.fasta --output data/processed/metadata.csv
python scripts/prepare_splits.py --metadata data/processed/metadata.csv --output data/processed/splits.csv
python -m src.embeddings.fetch_embeddings --embed2_backend protbert --max_len 300
python scripts/train.py --config configs/default.yaml
python scripts/evaluate.py --checkpoint results/checkpoints/best_model.pt --config configs/default.yaml
\end{verbatim}

\section{Conclusion}
Le pipeline complet est operationnel avec des contraintes CPU fortes. L'architecture respecte la contrainte
\emph{embeddings + Transformer + MLP}. Les resultats actuels sont solides compte tenu du materiel disponible.
