\section{Contexte et objectif}
L'objectif est de predire la localisation subcellulaire de proteines procaryotes (6 classes)
a partir de sequences FASTA. Le projet impose une approche plus originale qu'un simple MLP, avec
une architecture qui utilise des embeddings, une couche Transformer, puis un MLP de classification.

\section{Contraintes et environnement}
Le travail a ete realise sur un ASUS TUF 15, majoritairement en CPU. Cela impose des limites de memoire et
un temps de calcul eleve pour l'extraction des embeddings. Pour rester dans ces contraintes:
\begin{itemize}
  \item limitation de la longueur maximale (\texttt{max\_len}) pour eviter les OOM ;
  \item utilisation de ProstT5 3Di en mode CPU avec \textit{offload disque} ;
  \item cache local des modeles HF pour eviter les retelechargements ;
  \item batch sizes ajustes (ESM-C modere, ProstT5 a batch 1).
\end{itemize}

\section{Donnees}
Les donnees proviennent de DeepLocPro, fournies au format FASTA. Le header suit le format:
\begin{verbatim}
>PROTEIN_ID|LOCATION|GRAM_TYPE|PARTITION
\end{verbatim}
Nous generons un fichier \texttt{metadata.csv} puis des splits \texttt{train/val/test}. Par defaut, les splits
sont stratifies, avec une option pour clustering par identite si MMseqs2 est installe.

\section{Pipeline de traitement}
\begin{enumerate}
  \item \textbf{Preparation des metadonnees} a partir du FASTA.
  \item \textbf{Creation des splits} leakage-safe.
  \item \textbf{Extraction des embeddings}:
    \begin{itemize}
      \item ESM-C (300M) pour un embedding principal (dimension 960).
      \item ProstT5 3Di (dimension 1024) comme second embedding.
    \end{itemize}
  \item \textbf{Entrainement} du modele Transformer + BiLSTM + MLP.
  \item \textbf{Evaluation} sur le split test (Accuracy, F1 macro, MCC).
\end{enumerate}

\section{Choix des embeddings et reduction du cout}
Le sujet impose l'utilisation conjointe de ESM-C et ProstT5 3Di. ProstT5 etant couteux sur CPU,
nous avons active l'\textit{offload} disque et adopte un batch de 1 pour stabiliser l'extraction.

Pour limiter le temps de calcul tout en gardant une couverture elevee du dataset, \texttt{max\_len}
est fixe a 1000. Cette valeur conserve la grande majorite des sequences, tout en restant praticable
sur une machine CPU.

\section{Architecture du modele}
Le modele propose une architecture plus originale que l'article:
\emph{embeddings $\rightarrow$ Transformer $\rightarrow$ BiLSTM $\rightarrow$ MLP}.
\begin{itemize}
  \item projection lineaire des deux embeddings vers une dimension commune ;
  \item traitement des modalites comme tokens d'un Transformer encoder ;
  \item passage dans un BiLSTM pour modeliser les interactions ordonnees entre tokens ;
  \item fusion \texttt{CLS + attention-pooling LSTM} puis MLP de classification.
\end{itemize}

\subsection*{Configuration principale}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Parametre & Valeur \\
\midrule
Embedding 1 & ESM-C 300M (dim 960) \\
Embedding 2 & ProstT5 3Di (dim 1024) \\
Pooling & meanpool \\
max\_len & 1000 \\
ESM batch & 16 \\
Embed2 batch & 1 \\
\bottomrule
\end{tabular}
\caption{Configuration principale pour l'extraction et l'entrainement.}
\end{table}

\section{Entrainement}
La perte utilisee est la \emph{focal loss} avec un echantillonnage equilibre pour gerer l'imbalance des classes.
L'entrainement utilise un early stopping base sur le F1 macro.

\section{Exploration des donnees (EDA)}
Le dataset contient 11\,906 sequences. Les colonnes principales sont: \texttt{protein\_id}, \texttt{sequence},
\texttt{label}, \texttt{gram\_type}, \texttt{partition}, \texttt{split}, \texttt{seq\_len}. Aucune valeur manquante
n'a ete observee.

\subsection*{Distribution des classes}
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Classe & Nombre \\
\midrule
Cytoplasmic & 6885 \\
Cytoplasmic Membrane & 2535 \\
Extracellular & 1077 \\
Outer Membrane & 756 \\
Periplasmic & 566 \\
Cell Wall & 87 \\
\bottomrule
\end{tabular}
\caption{Distribution des classes (EDA).}
\end{table}

\subsection*{Statistiques de longueur}
La longueur des sequences est tres variable (distribution a longue traine). Statistiques principales:
\begin{itemize}
  \item moyenne: 438.4, ecart-type: 289.0
  \item min: 8, mediane: 399, max: 5627
  \item P10: 146, P25: 264, P75: 557, P90: 787, P95: 878, P99: 1296
\end{itemize}
Pour des tests rapides sur CPU, un \texttt{max\_len} entre 600 et 1000 est recommande.
La valeur \texttt{max\_len=1000} conserve la majorite des sequences tout en restant calculable.

\subsection*{Repartition des splits}
Train: 8333, Val: 1191, Test: 2382.

\subsection*{Graphiques}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{overview.png}
\caption{Vue d'ensemble: labels, longueurs, et splits.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{sequence_length_hist.png}
\caption{Distribution des longueurs de sequences.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{split_distribution.png}
\caption{Repartition train/val/test.}
\end{figure}

\section{Resultats}
Run utilise: ESM-C + ProstT5 3Di (meanpool), \texttt{max\_len=1000}, execution CPU.

\subsection*{Metriques globales}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Run & Accuracy & F1 macro & MCC \\
\midrule
Evaluation & 0.8741 & 0.8005 & 0.8227 \\
\bottomrule
\end{tabular}
\caption{Metriques globales sur le split test.}
\end{table}

\subsection*{Resultats par classe}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Classe & Precision & Recall & F1 \\
\midrule
Cytoplasmic & 0.901 & 0.846 & 0.873 \\
Cytoplasmic Membrane & 0.500 & 0.750 & 0.600 \\
Periplasmic & 0.933 & 0.925 & 0.929 \\
Outer Membrane & 0.776 & 0.839 & 0.806 \\
Extracellular & 0.706 & 0.818 & 0.758 \\
Cell Wall & 0.857 & 0.818 & 0.837 \\
\bottomrule
\end{tabular}
\caption{Precision/Recall/F1 par classe (split test).}
\end{table}

\begin{figure}[H]
\centering
\IfFileExists{figures/results_metrics.png}{
  \includegraphics[width=0.7\linewidth]{results_metrics.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure non disponible: lancer l'evaluation puis \texttt{make results-figures}.}}
}
\caption{Metriques globales.}
\end{figure}

\begin{figure}[H]
\centering
\IfFileExists{figures/results_per_class_f1.png}{
  \includegraphics[width=0.8\linewidth]{results_per_class_f1.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure non disponible: lancer l'evaluation puis \texttt{make results-figures}.}}
}
\caption{F1 par classe.}
\end{figure}

\begin{figure}[H]
\centering
\IfFileExists{figures/confusion_matrix_approx.png}{
  \includegraphics[width=0.9\linewidth]{confusion_matrix_approx.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure non disponible: regenerer les figures de resultats.}}
}
\caption{Matrice de confusion \textbf{approximative} reconstruite a partir des metriques agregees du rapport (ce n'est pas la matrice brute issue des predictions).}
\end{figure}

\section{Limites et pistes d'amelioration}
\begin{itemize}
  \item Monter \texttt{max\_len} et/ou utiliser un GPU pour integrer plus d'information de sequence.
  \item Tester une variante sans BiLSTM pour mesurer son apport exact.
  \item Augmenter la profondeur du Transformer ou tester un pooling different (mean/cls/both).
  \item Etendre l'evaluation avec validation croisee stricte type GraphPart.
\end{itemize}

\section{Reproductibilite}
Commandes principales (extraits):
\begin{verbatim}
python scripts/prepare_metadata.py --fasta data/raw/graphpart_set.fasta --output data/processed/metadata.csv
python scripts/prepare_splits.py --metadata data/processed/metadata.csv --output data/processed/splits.csv
python -m src.embeddings.fetch_embeddings --embed2_backend prostt5 --max_len 1000 --prost_batch 1 --prost_offload_dir data/interim/offload --prost_max_memory 6GB
python scripts/train.py --config configs/default.yaml
python scripts/evaluate.py --checkpoint results/checkpoints/best_model.pt --config configs/default.yaml
make eda
make results-figures
\end{verbatim}

\section{Conclusion}
Le pipeline complet est operationnel avec des contraintes CPU fortes. L'architecture respecte la contrainte
\emph{embeddings + Transformer + MLP}. Les resultats actuels sont solides compte tenu du materiel disponible.
