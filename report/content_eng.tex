\section{Goal}
We predict the subcellular localization of prokaryotic proteins (6 classes) from FASTA sequences.
The project requires an architecture more advanced than a plain MLP, using embeddings, a Transformer
fusion block, a BiLSTM refinement, and a classification MLP.

\section{Constraints and environment}
Experiments were run on an ASUS TUF 15, mostly on CPU. This creates memory and time constraints
for embedding extraction. To stay within limits we:
\begin{itemize}
  \item cap sequence length (\texttt{max\_len}) to avoid OOM;
  \item keep ProstT5 3Di in CPU mode with disk offload;
  \item cache HF models locally to avoid re-downloads;
  \item tune conservative batch sizes for stability.
\end{itemize}

\section{Data}
Data comes from DeepLocPro in FASTA format. Headers follow:
\begin{verbatim}
>PROTEIN_ID|LOCATION|GRAM_TYPE|PARTITION
\end{verbatim}
We generate \texttt{metadata.csv} and leakage-safe train/val/test splits (stratified by default).

\section{Pipeline}
\begin{enumerate}
  \item \textbf{Metadata preparation} from FASTA.
  \item \textbf{Split creation} with leakage control.
  \item \textbf{Embedding extraction}:
    \begin{itemize}
      \item ESM-C (300M) as main embedding (dim 960).
      \item ProstT5 3Di (dim 1024) as required second embedding.
    \end{itemize}
  \item \textbf{Training} of Transformer + BiLSTM + MLP model.
  \item \textbf{Evaluation} on test split (Accuracy, Macro-F1, MCC).
\end{enumerate}

\section{Embedding choice and compute budget}
The assignment requires ESM-C + ProstT5 3Di. ProstT5 is expensive on CPU, so we used
disk offload and a batch size of 1 for stable extraction.

To keep extraction feasible, we set \texttt{max\_len=1000}. This keeps most sequences while
remaining practical on CPU.

\section{Model architecture}
Our model uses \emph{embeddings $\rightarrow$ Transformer $\rightarrow$ BiLSTM $\rightarrow$ MLP}:
\begin{itemize}
  \item linear projection of both embeddings into a common space;
  \item encode modality tokens with a small Transformer encoder;
  \item process encoded tokens with BiLSTM and attention pooling;
  \item fuse pooled LSTM output with \texttt{[CLS]} before MLP classification.
\end{itemize}

\subsection*{Main configuration}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Embedding 1 & ESM-C 300M (dim 960) \\
Embedding 2 & ProstT5 3Di (dim 1024) \\
Pooling & meanpool \\
max\_len & 1000 \\
ESM batch & 16 \\
Embed2 batch & 1 \\
\bottomrule
\end{tabular}
\caption{Main extraction and training configuration.}
\end{table}

\section{Training}
We use focal loss with class weighting for imbalance and early stopping on Macro-F1.

\section{EDA summary}
The dataset has 11,906 sequences. Main columns: \texttt{protein\_id}, \texttt{sequence}, \texttt{label},
\texttt{gram\_type}, \texttt{partition}, \texttt{split}, \texttt{seq\_len}. No missing values observed.

\subsection*{Class distribution}
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Class & Count \\
\midrule
Cytoplasmic & 6885 \\
Cytoplasmic Membrane & 2535 \\
Extracellular & 1077 \\
Outer Membrane & 756 \\
Periplasmic & 566 \\
Cell Wall & 87 \\
\bottomrule
\end{tabular}
\caption{Class distribution (EDA).}
\end{table}

\subsection*{Sequence length stats}
\begin{itemize}
  \item mean: 438.4, std: 289.0
  \item min: 8, median: 399, max: 5627
  \item P10: 146, P25: 264, P75: 557, P90: 787, P95: 878, P99: 1296
\end{itemize}
For CPU runs, \texttt{max\_len} between 600 and 1000 is recommended.
With \texttt{max\_len=1000}, most sequences are kept while compute remains manageable.

\subsection*{Split sizes}
Train: 8333, Val: 1191, Test: 2382.

\subsection*{Figures}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{overview.png}
\caption{Overview: labels, lengths, splits.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{sequence_length_hist.png}
\caption{Sequence length distribution.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{split_distribution.png}
\caption{Train/Val/Test split.}
\end{figure}

\section{Results}
Run configuration: ESM-C + ProstT5 3Di (meanpool), \texttt{max\_len=1000}, CPU execution.

\subsection*{Global metrics}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Run & Accuracy & Macro F1 & MCC \\
\midrule
Evaluation & 0.8741 & 0.8005 & 0.8227 \\
\bottomrule
\end{tabular}
\caption{Global test metrics.}
\end{table}

\subsection*{Per-class metrics}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Class & Precision & Recall & F1 \\
\midrule
Cytoplasmic & 0.901 & 0.846 & 0.873 \\
Cytoplasmic Membrane & 0.500 & 0.750 & 0.600 \\
Periplasmic & 0.933 & 0.925 & 0.929 \\
Outer Membrane & 0.776 & 0.839 & 0.806 \\
Extracellular & 0.706 & 0.818 & 0.758 \\
Cell Wall & 0.857 & 0.818 & 0.837 \\
\bottomrule
\end{tabular}
\caption{Per-class precision/recall/F1.}
\end{table}

\begin{figure}[H]
\centering
\IfFileExists{figures/results_metrics.png}{
  \includegraphics[width=0.7\linewidth]{results_metrics.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure not available: run evaluation then \texttt{make results-figures}.}}
}
\caption{Global metrics.}
\end{figure}

\begin{figure}[H]
\centering
\IfFileExists{figures/results_per_class_f1.png}{
  \includegraphics[width=0.8\linewidth]{results_per_class_f1.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure not available: run evaluation then \texttt{make results-figures}.}}
}
\caption{Per-class F1.}
\end{figure}

\begin{figure}[H]
\centering
\IfFileExists{figures/confusion_matrix_approx.png}{
  \includegraphics[width=0.9\linewidth]{confusion_matrix_approx.png}
}{
  \fbox{\parbox{0.8\linewidth}{\centering Figure not available: regenerate result figures.}}
}
\caption{Approximate confusion matrix reconstructed from aggregated report metrics (not the exact matrix computed from raw predictions).}
\end{figure}

\section{Limitations and future work}
\begin{itemize}
  \item Increase \texttt{max\_len} or use a GPU to retain more sequence information.
  \item Quantify the exact gain of BiLSTM versus pure Transformer fusion.
  \item Explore deeper Transformer fusion or alternative pooling strategies.
\end{itemize}

\section{Reproducibility}
Main commands:
\begin{verbatim}
python scripts/prepare_metadata.py --fasta data/raw/graphpart_set.fasta --output data/processed/metadata.csv
python scripts/prepare_splits.py --metadata data/processed/metadata.csv --output data/processed/splits.csv
python -m src.embeddings.fetch_embeddings --embed2_backend prostt5 --max_len 1000 --prost_batch 1 --prost_offload_dir data/interim/offload --prost_max_memory 6GB
python scripts/train.py --config configs/default.yaml
python scripts/evaluate.py --checkpoint results/checkpoints/best_model.pt --config configs/default.yaml
make eda
make results-figures
\end{verbatim}

\section{Conclusion}
The end-to-end pipeline is operational under CPU constraints and uses a custom
\emph{embeddings + Transformer + BiLSTM + MLP} architecture beyond the original paper.
