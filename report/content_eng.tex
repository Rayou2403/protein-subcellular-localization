\section{Goal}
We predict the subcellular localization of prokaryotic proteins (6 classes) from FASTA sequences.
The project requires an architecture more advanced than a plain MLP, using embeddings, a Transformer
fusion block, and a classification MLP.

\section{Constraints and environment}
Experiments were run on an ASUS TUF 15, mostly on CPU. This creates memory and time constraints
for embedding extraction. To stay within limits we:
\begin{itemize}
  \item cap sequence length (\texttt{max\_len}) to avoid OOM;
  \item use a lighter second embedding (ProtBert) on CPU because ProstT5 is too heavy without GPU;
  \item cache HF models locally to avoid re-downloads;
  \item tune batch sizes for stability.
\end{itemize}

\section{Data}
Data comes from DeepLocPro in FASTA format. Headers follow:
\begin{verbatim}
>PROTEIN_ID|LOCATION|GRAM_TYPE|PARTITION
\end{verbatim}
We generate \texttt{metadata.csv} and leakage-safe train/val/test splits (stratified by default).

\section{Pipeline}
\begin{enumerate}
  \item \textbf{Metadata preparation} from FASTA.
  \item \textbf{Split creation} with leakage control.
  \item \textbf{Embedding extraction}:
    \begin{itemize}
      \item ESM-C (300M) as main embedding (dim 960).
      \item ProtBert (dim 1024) as CPU-friendly second embedding.
    \end{itemize}
  \item \textbf{Training} of Transformer + MLP model.
  \item \textbf{Evaluation} on test split (Accuracy, Macro-F1, MCC).
\end{enumerate}

\section{Embedding choice and compute budget}
ESM-C + ProtBert is a quality/compute trade-off. ESM-C gives strong representations, while ProstT5
requires large memory (model > 11GB) and is not viable on CPU. ProtBert is much lighter and stable.

To keep extraction feasible, we set \texttt{max\_len=300}. This retains roughly 30\% of sequences and
keeps CPU runs stable. With a GPU, \texttt{max\_len} can be increased to keep more data.

\section{Model architecture}
The model follows the required \emph{embeddings $\rightarrow$ Transformer $\rightarrow$ MLP} pipeline:
\begin{itemize}
  \item linear projection of both embeddings into a common space;
  \item treat the two modalities as two tokens in a small Transformer encoder;
  \item classify from the \texttt{[CLS]} token through an MLP head.
\end{itemize}

\subsection*{Main configuration}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Embedding 1 & ESM-C 300M (dim 960) \\
Embedding 2 & ProtBert (dim 1024) \\
Pooling & meanpool \\
max\_len & 300 \\
ESM batch & 32 \\
Embed2 batch & 32 \\
\bottomrule
\end{tabular}
\caption{Main extraction and training configuration.}
\end{table}

\section{Training}
We use focal loss with class weighting for imbalance and early stopping on Macro-F1.

\section{EDA summary}
The dataset has 11,906 sequences. Main columns: \texttt{protein\_id}, \texttt{sequence}, \texttt{label},
\texttt{gram\_type}, \texttt{partition}, \texttt{split}, \texttt{seq\_len}. No missing values observed.

\subsection*{Class distribution}
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Class & Count \\
\midrule
Cytoplasmic & 6885 \\
Cytoplasmic Membrane & 2535 \\
Extracellular & 1077 \\
Outer Membrane & 756 \\
Periplasmic & 566 \\
Cell Wall & 87 \\
\bottomrule
\end{tabular}
\caption{Class distribution (EDA).}
\end{table}

\subsection*{Sequence length stats}
\begin{itemize}
  \item mean: 438.4, std: 289.0
  \item min: 8, median: 399, max: 5627
  \item P10: 146, P25: 264, P75: 557, P90: 787, P95: 878, P99: 1296
\end{itemize}
For CPU runs, \texttt{max\_len} between 150 and 300 is recommended.
With \texttt{max\_len=300}, roughly 30\% of sequences are kept, which keeps compute manageable.

\subsection*{Split sizes}
Train: 8929, Val: 1191, Test: 1786.

\subsection*{Figures}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{overview.png}
\caption{Overview: labels, lengths, splits.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{sequence_length_hist.png}
\caption{Sequence length distribution.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\linewidth]{split_distribution.png}
\caption{Train/Val/Test split.}
\end{figure}

\section{Results}
Run configuration: ESM-C + ProtBert (meanpool), \texttt{max\_len=300}, CPU execution.

\subsection*{Global metrics}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Run & Accuracy & Macro F1 & MCC \\
\midrule
Evaluation & 0.8741 & 0.8005 & 0.8227 \\
\bottomrule
\end{tabular}
\caption{Global test metrics.}
\end{table}

\subsection*{Per-class metrics}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Class & Precision & Recall & F1 \\
\midrule
Cytoplasmic & 0.901 & 0.846 & 0.873 \\
Cytoplasmic Membrane & 0.500 & 0.750 & 0.600 \\
Periplasmic & 0.933 & 0.925 & 0.929 \\
Outer Membrane & 0.776 & 0.839 & 0.806 \\
Extracellular & 0.706 & 0.818 & 0.758 \\
Cell Wall & 0.857 & 0.818 & 0.837 \\
\bottomrule
\end{tabular}
\caption{Per-class precision/recall/F1.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{results_metrics.png}
\caption{Global metrics.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{results_per_class_f1.png}
\caption{Per-class F1.}
\end{figure}

\section{Limitations and future work}
\begin{itemize}
  \item Increase \texttt{max\_len} or use a GPU to retain more sequence information.
  \item Replace ProtBert with ProstT5 on GPU for structure-aware embeddings.
  \item Explore deeper Transformer fusion or alternative pooling strategies.
\end{itemize}

\section{Reproducibility}
Main commands:
\begin{verbatim}
python scripts/prepare_metadata.py --fasta data/raw/graphpart_set.fasta --output data/processed/metadata.csv
python scripts/prepare_splits.py --metadata data/processed/metadata.csv --output data/processed/splits.csv
python -m src.embeddings.fetch_embeddings --embed2_backend protbert --max_len 300
python scripts/train.py --config configs/default.yaml
python scripts/evaluate.py --checkpoint results/checkpoints/best_model.pt --config configs/default.yaml
\end{verbatim}

\section{Conclusion}
The end-to-end pipeline is operational under CPU constraints and respects the required
\emph{embeddings + Transformer + MLP} architecture. The current results are strong given the hardware limits.
